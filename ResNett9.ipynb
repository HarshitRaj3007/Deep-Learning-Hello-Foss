{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aryangoyal7/Deep-Learning-Hello-Foss/blob/main/ResNett9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Doing the neccessary imports"
      ],
      "metadata": {
        "id": "EHw6yVN7OnJU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4tyH9qt7BtmR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torchvision.datasets import ImageFolder\n",
        "\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's import the data, there are multiple ways of doing it, we prefer this one"
      ],
      "metadata": {
        "id": "MXn6Vn33Ov8k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opendatasets --upgrade --quiet\n",
        "import opendatasets as od"
      ],
      "metadata": {
        "id": "JxaW7cAjB1AL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_url = 'https://www.kaggle.com/datasets/aneesh10/cricket-shot-dataset'\n",
        "\n",
        "#1e843ab47f3b61961799fbfc3be0f1b7\n"
      ],
      "metadata": {
        "id": "IKrcmU1GB30o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "od.download(dataset_url)\n"
      ],
      "metadata": {
        "id": "x6Av9jhhB4t6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86e358f0-7f11-4b1f-bc93-9bda91fb0186"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
            "Your Kaggle username: aryanx07\n",
            "Your Kaggle Key: ··········\n",
            "Downloading cricket-shot-dataset.zip to ./cricket-shot-dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 645M/645M [00:17<00:00, 38.9MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = './cricket-shot-dataset/data'\n",
        "import os"
      ],
      "metadata": {
        "id": "-PStJ-ZdB4yI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir(data_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXq8VFtgB41L",
        "outputId": "4510edfa-0849-4ac0-91dc-7d65912e65da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['legglance-flick', 'drive', 'pullshot', 'sweep']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So we have 4 classes of shots, we have to classify each image as one of these shots"
      ],
      "metadata": {
        "id": "Ev9fWwbCO-Fe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = ImageFolder(data_dir)"
      ],
      "metadata": {
        "id": "Zb5W-t8xCX9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDvnM4QQ-hlm",
        "outputId": "592a03bc-e7e6-4f6a-9806-dc3511b6d5e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4724"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's perform the necessary transformations over the dataset images before loading them into our model\n",
        "</br>\n",
        "-> Normalization is a process that changes the range of pixel intensity values"
      ],
      "metadata": {
        "id": "6mdYMHONPMku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as tt\n",
        "#Normalization\n",
        "# calculate mean and standard deviation from each channel and correct it here for better results\n",
        "normalize = tt.Normalize(\n",
        "        mean=[0.5, 0.5, 0.5],\n",
        "        std=[0.2, 0.2, 0.2],\n",
        ")\n",
        "#Resizing,cropping and converting into tensor\n",
        "dataset = ImageFolder(data_dir, tt.Compose([tt.Resize(64), \n",
        "                                            tt.RandomCrop(64), \n",
        "                                            tt.ToTensor()]))"
      ],
      "metadata": {
        "id": "bFZDlliwCyiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll split the dataset into 2 parts, training and validation"
      ],
      "metadata": {
        "id": "uvms2BjZPvnW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val_pct = 0.1\n",
        "val_size = int(val_pct * len(dataset))\n",
        "train_size = len(dataset) - val_size\n",
        "\n",
        "train_size, val_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73prC60eCyta",
        "outputId": "1ae79289-4f82-4d75-85cf-b85d92bb241c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4252, 472)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import random_split\n",
        "\n",
        "train_ds, valid_ds = random_split(dataset, [train_size, val_size])\n",
        "len(train_ds), len(valid_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fCt4e48LDARQ",
        "outputId": "e29a9115-62c5-4e5d-9ca0-9a945ee41b75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4252, 472)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll load the datasets in batches, but what is __batch size__?\n",
        "</br>\n",
        "The batch size defines the number of samples that will be propagated through the network.\n",
        "</br> \n",
        "For example you have a dataset of 100 images , you set the batch size to 10, it will pass the first 10 images through the network then train the model, the process is repeated for all the batches."
      ],
      "metadata": {
        "id": "4juf8Uw8Q6tE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "train_dl = DataLoader(train_ds, \n",
        "                      batch_size, \n",
        "                      shuffle=True, \n",
        "                      num_workers=4, \n",
        "                      pin_memory=True)\n",
        "\n",
        "valid_dl = DataLoader(valid_ds, \n",
        "                    batch_size, \n",
        "                    num_workers=4, \n",
        "                    pin_memory=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-7J1v44DFup",
        "outputId": "cdcbabf6-86eb-4af8-8529-d31bc4e1ba43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is our model, we are using a standard ResNet9 model, read more about it here [link] - 'resnet9' link"
      ],
      "metadata": {
        "id": "qLF_2FHk7GT3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def conv_block(in_channels, out_channels, pool=False):\n",
        "    layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1), \n",
        "              nn.BatchNorm2d(out_channels), \n",
        "              nn.ReLU(inplace=True)]\n",
        "    if pool: layers.append(nn.MaxPool2d(2))\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "class ResNet9(nn.Module):\n",
        "    def __init__(self, in_channels, num_classes):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.conv1 = conv_block(in_channels, 64) \n",
        "        self.conv2 = conv_block(64, 128, pool=True) \n",
        "        self.res1 = nn.Sequential(conv_block(128, 128), \n",
        "                                  conv_block(128, 128))\n",
        "        \n",
        "        self.conv3 = conv_block(128, 256, pool=True) \n",
        "        self.conv4 = conv_block(256, 512, pool=True) \n",
        "        self.res2 = nn.Sequential(conv_block(512, 512),  \n",
        "                                  conv_block(512, 512)) \n",
        "        \n",
        "        self.classifier = nn.Sequential(nn.AdaptiveMaxPool2d(1), \n",
        "                                        nn.Flatten(), \n",
        "                                        nn.Dropout(0.2),\n",
        "                                        nn.Linear(512, num_classes))\n",
        "        \n",
        "    def forward(self, xb):\n",
        "        out = self.conv1(xb)\n",
        "        out = self.conv2(out)\n",
        "        out = self.res1(out) + out\n",
        "        out = self.conv3(out)\n",
        "        out = self.conv4(out)\n",
        "        out = self.res2(out) + out\n",
        "        out = self.classifier(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "TvEI5m6UDIfu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are some of the hyperparameters of our model, they have a good role to play in determining the accuracy of our model, try changing them for better accuracy"
      ],
      "metadata": {
        "id": "2xTGf_Wm7ZvG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 4\n",
        "num_epochs = 30\n",
        "batch_size = 16\n",
        "learning_rate = 0.005\n",
        "in_channels = 3\n",
        "model = ResNet9(3,4).to(device)\n",
        "\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay = 0.005, momentum = 0.9)  \n",
        "\n",
        "\n",
        "# Train the model\n",
        "total_step = len(train_dl)"
      ],
      "metadata": {
        "id": "9F7xKuj_DRBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_step = len(train_dl)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_dl):  \n",
        "        # Move tensors to the configured device\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
        "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
        "            \n",
        "    # Validation\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for images, labels in valid_dl:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            del images, labels, outputs\n",
        "    \n",
        "        print('Accuracy of the network on the {} validation images: {} %'.format(4724, 100 * correct / total)) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1TOrhUqDVYx",
        "outputId": "f605dc7b-6c32-4ad9-a0b0-55205964a79e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/30], Step [34/34], Loss: 1.6213\n",
            "Accuracy of the network on the 4724 validation images: 26.483050847457626 %\n",
            "Epoch [2/30], Step [34/34], Loss: 2.3673\n",
            "Accuracy of the network on the 4724 validation images: 27.33050847457627 %\n",
            "Epoch [3/30], Step [34/34], Loss: 1.1373\n",
            "Accuracy of the network on the 4724 validation images: 30.93220338983051 %\n",
            "Epoch [4/30], Step [34/34], Loss: 1.1467\n",
            "Accuracy of the network on the 4724 validation images: 47.03389830508475 %\n",
            "Epoch [5/30], Step [34/34], Loss: 1.0356\n",
            "Accuracy of the network on the 4724 validation images: 42.79661016949152 %\n",
            "Epoch [6/30], Step [34/34], Loss: 1.5511\n",
            "Accuracy of the network on the 4724 validation images: 37.5 %\n",
            "Epoch [7/30], Step [34/34], Loss: 1.0742\n",
            "Accuracy of the network on the 4724 validation images: 57.41525423728814 %\n",
            "Epoch [8/30], Step [34/34], Loss: 0.8544\n",
            "Accuracy of the network on the 4724 validation images: 54.23728813559322 %\n",
            "Epoch [9/30], Step [34/34], Loss: 0.8991\n",
            "Accuracy of the network on the 4724 validation images: 65.67796610169492 %\n",
            "Epoch [10/30], Step [34/34], Loss: 0.5616\n",
            "Accuracy of the network on the 4724 validation images: 60.16949152542373 %\n",
            "Epoch [11/30], Step [34/34], Loss: 0.8941\n",
            "Accuracy of the network on the 4724 validation images: 71.82203389830508 %\n",
            "Epoch [12/30], Step [34/34], Loss: 0.6816\n",
            "Accuracy of the network on the 4724 validation images: 66.52542372881356 %\n",
            "Epoch [13/30], Step [34/34], Loss: 0.9040\n",
            "Accuracy of the network on the 4724 validation images: 75.21186440677967 %\n",
            "Epoch [14/30], Step [34/34], Loss: 0.5617\n",
            "Accuracy of the network on the 4724 validation images: 70.33898305084746 %\n",
            "Epoch [15/30], Step [34/34], Loss: 1.2107\n",
            "Accuracy of the network on the 4724 validation images: 78.38983050847457 %\n",
            "Epoch [16/30], Step [34/34], Loss: 0.2148\n",
            "Accuracy of the network on the 4724 validation images: 78.38983050847457 %\n",
            "Epoch [17/30], Step [34/34], Loss: 0.8408\n",
            "Accuracy of the network on the 4724 validation images: 85.80508474576271 %\n",
            "Epoch [18/30], Step [34/34], Loss: 0.3405\n",
            "Accuracy of the network on the 4724 validation images: 77.33050847457628 %\n",
            "Epoch [19/30], Step [34/34], Loss: 0.1233\n",
            "Accuracy of the network on the 4724 validation images: 85.16949152542372 %\n",
            "Epoch [20/30], Step [34/34], Loss: 0.2247\n",
            "Accuracy of the network on the 4724 validation images: 88.55932203389831 %\n",
            "Epoch [21/30], Step [34/34], Loss: 0.1407\n",
            "Accuracy of the network on the 4724 validation images: 87.5 %\n",
            "Epoch [22/30], Step [34/34], Loss: 0.2844\n",
            "Accuracy of the network on the 4724 validation images: 87.71186440677967 %\n",
            "Epoch [23/30], Step [34/34], Loss: 0.0902\n",
            "Accuracy of the network on the 4724 validation images: 88.77118644067797 %\n",
            "Epoch [24/30], Step [34/34], Loss: 0.5267\n",
            "Accuracy of the network on the 4724 validation images: 90.2542372881356 %\n",
            "Epoch [25/30], Step [34/34], Loss: 0.3892\n",
            "Accuracy of the network on the 4724 validation images: 88.77118644067797 %\n",
            "Epoch [26/30], Step [34/34], Loss: 0.2726\n",
            "Accuracy of the network on the 4724 validation images: 91.94915254237289 %\n",
            "Epoch [27/30], Step [34/34], Loss: 0.0301\n",
            "Accuracy of the network on the 4724 validation images: 86.65254237288136 %\n",
            "Epoch [28/30], Step [34/34], Loss: 0.0863\n",
            "Accuracy of the network on the 4724 validation images: 91.52542372881356 %\n",
            "Epoch [29/30], Step [34/34], Loss: 0.0386\n",
            "Accuracy of the network on the 4724 validation images: 93.4322033898305 %\n",
            "Epoch [30/30], Step [34/34], Loss: 0.0947\n",
            "Accuracy of the network on the 4724 validation images: 92.58474576271186 %\n"
          ]
        }
      ]
    }
  ]
}